{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Description and Matching\n",
    "## SIFT\n",
    "\n",
    "Import the packages and load the image \"Book.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Book.jpg\")\n",
    "\n",
    "b,g,r = cv2.split(img) # Changing the order from bgr to rgb so that matplotlib can show it\n",
    "img = cv2.merge([r,g,b])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by converting the image to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "plt.imshow(gray, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will attempt to find the features and descriptors using [SIFT](https://docs.opencv.org/2.4/modules/nonfree/doc/feature_detection.html). In OpenCV we will do it by first creating a SIFT object and then use the detectAndCompute function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp, des = sift.detectAndCompute(gray, None) #Keypoints and descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the detected keypoints we use the function [drawKeyPoints](https://docs.opencv.org/master/d4/d5d/group__features2d__draw.html#ga5d2bafe8c1c45289bc3403a40fb88920). We scale the picture up a bit using \"figsize\", so it's easier to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_img = cv2.drawKeypoints(gray, kp, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(kp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the image \"More_books.jpg\" and convert it to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(\"More_books.jpg\")\n",
    "b,g,r = cv2.split(img2) # Changing the order from bgr to rgb so that matplotlib can show it\n",
    "img2 = cv2.merge([r,g,b])\n",
    "\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "plt.imshow(gray2, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the same SIFT method to find the keypoints and descriptors of this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp2, des2 = sift.detectAndCompute(gray2, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the descriptors of both images, let's see if we can match them and find the book from the first image in the second image.\n",
    "For this example we use Brute Force Matcher ([cv2.BFMatcher](https://docs.opencv.org/master/d3/da1/classcv_1_1BFMatcher.html)). We use the function cv2.match to find the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher()\n",
    "matches = bf.match(des, des2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function [cv2.drawMatches](https://docs.opencv.org/3.4/d4/d5d/group__features2d__draw.html) to display the result. We'd like to display the best of the matches. The matching is made using a distance measurement between the descriptors, the smaller the distance, the better the match. So we sort the \"matches\" array using the distance term and then we plot only the first 100 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "img3 = cv2.drawMatches(gray,kp,gray2,kp2,matches[:100],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most of the matches are correct, but with some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Using the same two images, try to use the SURF method instead of SIFT to find the keypoints and the descriptors. Does one of these methods give a better result than the other? Try plotting more than just the 100 matches and see if there is a difference between the two methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
