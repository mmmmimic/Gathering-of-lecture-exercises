{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Originally created for a previous version of the [02456-deep-learning](https://github.com/DeepLearningDTU/02456-deep-learning) course material, but [converted to PyTorch](https://github.com/pytorch/tutorials).\n",
    "See repos for credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies and supporting functions\n",
    "Loading dependancies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural networks 101\n",
    "\n",
    "Convolution neural networks are one of the most succesfull types of neural networks for image recognition and an integral part of reigniting the interest in neural networks. \n",
    "They are abel to extract structural relations in the data such as spatial in images or temporal in time series.\n",
    "\n",
    "In this lab we'll experiment with inserting 2D-convolution layers in the fully connected neural networks introduced in lab 1 (`1_Feedforward`).\n",
    "We'll further experiment with stacking of convolution layers, max pooling and strided convolutions which are all important techniques in current convolution neural network architectures.\n",
    "Lastly we'll try to visualize the learned convolution filters and try to understand what kind of features they learn to recognize.\n",
    "\n",
    "If you haven't watched Jason Yosinski and colleague [awesome video on visualizing convolutional networks](https://www.youtube.com/watch?v=AgkfIQ4IGaM) you defneatly should do so now.\n",
    "\n",
    "If you are unfamilar with the the convolution operation https://github.com/vdumoulin/conv_arithmetic have a nice visualization of different convolution variants.\n",
    "For a more indepth tutorial please see http://cs231n.github.io/convolutional-networks/ or http://neuralnetworksanddeeplearning.com/chap6.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: What are convolutional networks?\n",
    "\n",
    "ConvNets are in may respects very similar to the dense feedforward networks we saw previously:\n",
    " * The network is still organized into layers\n",
    " * Each layer is parameterized by weights and biases\n",
    " * Each layer has an element-wise non-linear transformation (activation function)\n",
    " * There are no cycles in the connections (more on this in later labs)\n",
    "\n",
    "*So what is the difference?*\n",
    "The networks we saw previously are called *dense* because each unit receives input from all the units in the previous layer.\n",
    "This is not the case for ConvNets.\n",
    "In ConvNets each unit is only connected to a small subset of the input units.\n",
    "This is called the *receptive field* of the unit.\n",
    "\n",
    "#### Let us look at a quick example.\n",
    "The input (green matrix) is `1x5x5` dimensional tensor - i.e. it has one `channel` (like a grayscale image) and the map is of size `5x5`.\n",
    "Let us define a `1x3x3` kernel (yellow submatrix).\n",
    "The kernel weights are indicated by red in the bottom right of each elment.\n",
    "The computation can be thought of as first an elementwise multiplication, and then summing the results.\n",
    "Here we use a stride of 1, as shown in this animation:\n",
    "\n",
    "<img src=\"images/convolutions.gif\" style=\"width: 400px;\"/> \n",
    "[GIF courtesy of [Stanford](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)]\n",
    "\n",
    "After having convolved the image we perform an elementwise non-linear transformation on the *convolved features*.\n",
    "In this example the input is a 2D *feature map* with depth 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "### Assignment 1.1: Manual calculations\n",
    "\n",
    "Perform the following computation, and write the result below.\n",
    "\n",
    "![](images/conv_exe.png)\n",
    "\n",
    "1. Manually convolve the input, and compute the convolved features. No padding and stride of 1.\n",
    "1. Perform `2x2` max pooling on the convolved features. Stride of 2.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n",
    "### Assignment 1.2: Output dimensionality\n",
    "\n",
    "Given the following 3D tensor input `(channel, height, width)` , a given amount (`channels_out`) of filters `(channels_in, filter_height, filter_width)`, stride `(height, width)` and padding `(height, width)`, calculate the output dimensionality if it's valid.\n",
    "\n",
    "1. input tensor with dimensionality (1, 28, 28) and 16 filters of size (1, 5, 5) with stride (1, 1) and padding (0, 0)\n",
    " * **Answer:** \n",
    "2. input tensor with dimensionality (2, 32, 32) and 24 filters of size (2, 3, 3) with stride (1, 1) and padding (0, 0)\n",
    " * **Answer:** \n",
    "3. input tensor with dimensionality (10, 32, 32) and 3 filters of size (10, 2, 2) with stride (2, 2) and padding (0, 0)\n",
    " * **Answer:** \n",
    "4. input tensor with dimensionality (11, 8, 16) and 7 filters of size (11, 3, 3) with stride (2, 2) and padding (1, 1)\n",
    " * **Answer:** \n",
    "5. input tensor with dimensionality (128, 256, 256) and 112 filters of size (128, 3, 3) with stride (1, 1) and padding (1, 1)\n",
    " * **Answer:** \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: MNIST\n",
    "\n",
    "The code below downloads and loads the same MNIST dataset as before.\n",
    "Note however that the data has a different shape this time, namely `(num_samples, num_channels, height, width)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the MNIST dataset, if you haven't already\n",
    "\n",
    "!if [ ! -f mnist.npz ]; then wget -N https://www.dropbox.com/s/qxywaq7nx19z72p/mnist.npz; else echo \"mnist.npz already downloaded\"; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD the mnist data. \n",
    "\n",
    "# To speed up training we'll only work on a subset of the data.\n",
    "# Note that we reshape the data from \n",
    "#  (nsamples, num_features) = (nsamples, nchannels*rows*cols)\n",
    "#    -> (nsamples, nchannels, rows, cols)\n",
    "# in order to retain the spatial arrangements of the pixels\n",
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "nchannels, rows, cols = 1, 28, 28\n",
    "\n",
    "x_train = data['X_train'][:1000].astype('float32')\n",
    "x_train = x_train.reshape((-1, nchannels, rows, cols))\n",
    "targets_train = data['y_train'][:1000].astype('int32')\n",
    "\n",
    "x_valid = data['X_valid'][:500].astype('float32')\n",
    "x_valid = x_valid.reshape((-1, nchannels, rows, cols))\n",
    "targets_valid = data['y_valid'][:500].astype('int32')\n",
    "\n",
    "x_test = data['X_test'][:500].astype('float32')\n",
    "x_test = x_test.reshape((-1, nchannels, rows, cols))\n",
    "targets_test = data['y_test'][:500].astype('int32')\n",
    "\n",
    "print(\"Information on dataset\")\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"targets_train\", targets_train.shape)\n",
    "print(\"x_valid\", x_valid.shape)\n",
    "print(\"targets_valid\", targets_valid.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"targets_test\", targets_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot a few MNIST examples\n",
    "\n",
    "idx, dim, classes = 0, 28, 10\n",
    "# create empty canvas\n",
    "canvas = np.zeros((dim*classes, classes*dim))\n",
    "\n",
    "# fill with tensors\n",
    "for i in range(classes):\n",
    "    for j in range(classes):\n",
    "        canvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = x_train[idx].reshape((dim, dim))\n",
    "        idx += 1\n",
    "\n",
    "# visualize matrix of tensors as gray scale image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a simple feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperameters of the model\n",
    "num_classes = 10\n",
    "channels = x_train.shape[1]\n",
    "height = x_train.shape[2]\n",
    "width = x_train.shape[3]\n",
    "num_filters_conv1 = 16\n",
    "kernel_size_conv1 = 5 # [height, width]\n",
    "stride_conv1 = 1 # [stride_height, stride_width]\n",
    "num_l1 = 100\n",
    "padding_conv1 = 0\n",
    "   \n",
    "def compute_conv_dim(dim_size):\n",
    "    return int((dim_size - kernel_size_conv1 + 2 * padding_conv1) / stride_conv1 + 1)\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # out_dim = (input_dim - filter_dim + 2padding) / stride + 1\n",
    "        # self.conv_1 = Conv2d(in_channels=channels,\n",
    "        #                     out_channels=num_filters_conv1,\n",
    "        #                     kernel_size=kernel_size_conv1,\n",
    "        #                     stride=stride_conv1)\n",
    "        \n",
    "        # self.conv_out_height = compute_conv_dim(height)\n",
    "        # self.conv_out_width = compute_conv_dim(width)\n",
    "        \n",
    "        # add dropout to network\n",
    "        # self.dropout = Dropout2d(p=0.5)\n",
    "        # self.l1_in_features = num_filters_conv1 * self.conv_out_height * self.conv_out_width\n",
    "        self.l1_in_features = channels * height * width\n",
    "        \n",
    "        self.l_1 = Linear(in_features=self.l1_in_features, \n",
    "                          out_features=num_l1,\n",
    "                          bias=True)\n",
    "        self.l_out = Linear(in_features=num_l1, \n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "    \n",
    "    def forward(self, x): # x.size() = [batch, channel, height, width]\n",
    "        #x = relu(self.conv_1(x))\n",
    "        # torch.Tensor.view: http://pytorch.org/docs/master/tensors.html?highlight=view#torch.Tensor.view\n",
    "        #   Returns a new tensor with the same data as the self tensor,\n",
    "        #   but of a different size.\n",
    "        # the size -1 is inferred from other dimensions \n",
    "        x = x.view(-1, self.l1_in_features)\n",
    "        #x = self.dropout(relu(self.l_1(x)))\n",
    "        x = relu(self.l_1(x))\n",
    "        return softmax(self.l_out(x), dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the forward pass with dummy data\n",
    "x = np.random.normal(0,1, (5, 1, 28, 28)).astype('float32')\n",
    "out = net(Variable(torch.from_numpy(x)))\n",
    "out.size(), out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output's probabilities are nicely distributed.\n",
    "The built-in nn functions (layers) alreay have a sane initialization of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have done this ourselves,\n",
    "# but we should be aware of sklearn and it's tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    cur_loss = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = Variable(torch.from_numpy(x_train[slce]))\n",
    "        output = net(x_batch)\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = Variable(torch.from_numpy(targets_train[slce]).long())\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_loss += batch_loss   \n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = Variable(torch.from_numpy(x_train[slce]))\n",
    "        \n",
    "        output = net(x_batch)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(targets_train[slce])\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = Variable(torch.from_numpy(x_valid[slce]))\n",
    "        \n",
    "        output = net(x_batch)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        val_targs += list(targets_valid[slce])\n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "        \n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Acc', 'Val Acc'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "\n",
    "### Evaluate test set\n",
    "x_batch = Variable(torch.from_numpy(x_test))\n",
    "output = net(x_batch)\n",
    "preds = torch.max(output, 1)[1]\n",
    "print(\"\\nTest set Acc:  %f\" % (accuracy_score(list(targets_test), list(preds.data.numpy()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "1. Note the performance of the standard feedforward neural network. Add a 2D convolution layer before the dense hidden layer and confirm that it increases the generalization performance of the network (try num_filters=16 and filter_size=5 as a starting point). \n",
    " \n",
    "2. Notice that the size of the image reduces. This can cause loss of information in convolutional networks that apply many convolutional layers. To avoid such add adequate padding to the convolutional layer.\n",
    " \n",
    "3. Can the performance be increases even further by stacking more convolution layers ?\n",
    " \n",
    "4. Maxpooling is a technique for decreasing the spatial resolution of an image while retaining the important features. Effectively this gives a local translational invariance and reduces the computation by a factor of four. In the classification algorithm which is usually desirable. Try to either: \n",
    " \n",
    "   - add a maxpool layer (add arguement kernel_size=2, stride=2) after the convolution layer, or\n",
    "   - set add stride=2 to the arguments of the convolution layer, make it fit with the kernel size\n",
    "     \n",
    "  Verify that this decreases spatial dimension of the image (`print(l_conv_x.size())` or `print(l_maxpool_x.size())` in your forward pass). Does this increase the performance of the network (you may need to stack multiple layers or increase the number of filters to increase performance) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of filters\n",
    "Convolution filters can be interpreted as spatial feature detectors picking up different image features such as edges, corners etc. Below we provide code for visualization of the filters. The best results are obtained with fairly large filters of size 9 and either 16 or 36 filters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start with we print the names of the weights in our network\n",
    "names_and_vars = {x[0]: x[1] for x in net.named_parameters()}\n",
    "print(names_and_vars.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ERROR - If you get a key error, then you need to define l_conv1 in your model!\n",
    "if not 'conv_1.weight' in names_and_vars:\n",
    "    print(\"You need to go back and define a convolutional layer in the network.\")\n",
    "else:\n",
    "    np_W = names_and_vars['conv_1.weight'].data.numpy() # get the filter values from the first conv layer\n",
    "    print(np_W.shape, \"i.e. the shape is (channels_out, channels_in, filter_height, filter_width)\")\n",
    "    channels_out, channels_in, filter_size, _ = np_W.shape\n",
    "    n = int(channels_out**0.5)\n",
    "\n",
    "    # reshaping the last dimension to be n by n\n",
    "    np_W_res = np_W.reshape(filter_size, filter_size, channels_in, n, n)\n",
    "    fig, ax = plt.subplots(n,n)\n",
    "    print(\"learned filter values\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i,j].imshow(np_W_res[:,:,0,i,j], cmap='gray',interpolation='none')\n",
    "            ax[i,j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "            ax[i,j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    idx = 1\n",
    "    plt.figure()\n",
    "    plt.imshow(x_train[idx,0],cmap='gray',interpolation='none')\n",
    "    plt.title('Inut Image')\n",
    "    plt.show()\n",
    "\n",
    "    #visalize the filters convolved with an input image\n",
    "    from scipy.signal import convolve2d\n",
    "    np_W_res = np_W.reshape(filter_size, filter_size, channels_in, n, n)\n",
    "    fig, ax = plt.subplots(n,n,figsize=(9,9))\n",
    "    print(\"Response from input image convolved with the filters\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i,j].imshow(convolve2d(x_train[1,0],np_W_res[:,:,0,i,j],mode='same'),\n",
    "                           cmap='gray',interpolation='none')\n",
    "            ax[i,j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "            ax[i,j].yaxis.set_major_formatter(plt.NullFormatter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "The visualized filters will likely look most like noise due to the small amount of training data.\n",
    "\n",
    "1. Try to use 10000 traning examples instead and visualise the filters again\n",
    " \n",
    "2. Dropout is a very usefull technique for preventing overfitting. Try to add a DropoutLayer after the convolution layer and hidden layer. This should increase both performance and the \"visual appeal\" of the filters\n",
    "   - remember to use `net.train()` and `net.eval()` properly.\n",
    " \n",
    "3. Batch normalization is a recent innovation for improving generalization performance. Try to insert batch normalization layers into the network to improve performance. \n",
    "   - remember to use `net.train()` and `net.eval()` properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if you didn't already, you really should [watch this video](https://www.youtube.com/watch?v=AgkfIQ4IGaM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
