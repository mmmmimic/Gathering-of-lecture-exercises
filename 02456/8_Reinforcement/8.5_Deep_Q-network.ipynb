{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve frozen lake with Deep Q-Network\n",
    "\n",
    "> By Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk))\n",
    "\n",
    "A big breakthrough in deep reinforcement learning happened in 2013 when DeepMind proposed a method called [Deep Q-network (DQN)](https://arxiv.org/abs/1312.5602) for training a neural network to play classic Atari games.\n",
    "The method was described further in [Nature](https://www.nature.com/articles/nature14236) in 2015.\n",
    "DQN is in many ways similar to the Q-network approach we saw in the previous notebook, but is extended with a few clever ideas to make it train better and more efficient. \n",
    "\n",
    "Two important additions were the use of an *experience replay memory* and a *target network*.\n",
    "The experience replay memory is used to store previous state-actions transitions that can then be sampled in mini-batches and used for training.\n",
    "This speeds up the training process by being more sample efficient and helps to break correlation between the sequentially observed experiences.\n",
    "Sampling experiences in this way turns the method from an on-policy into an off-policy method. \n",
    "The target network uses a separate neural network to estimate the target values used in the update step.\n",
    "As we saw in the previous notebook, the Q-network was also used to estimate the target values, which can make learning unstable as the network, and thus the targets, are updated frequently.\n",
    "The target network is often just a previous version of the Q-network that is update less often to make the estimates more stable. \n",
    "\n",
    "Solve the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment using the Deep Q-Network (DQN) method with epsilon-greedy action selection, experience replay and a target network.\n",
    "\n",
    "Just like the Q-Network, this approach employs a Q-function which can be updated using bootstrapping:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta) \\leftarrow Q(s,a;\\theta) + \\alpha \\delta \\ ,\n",
    "$$\n",
    "\n",
    "where $\\delta = r + \\gamma \\max_{a'} Q(s',a';\\theta') - Q(s,a;\\theta)$ is the TD error.\n",
    "\n",
    "Since we use a neural network to represent $Q$, we cannot do this assignment directly, but instead perform a gradient update using sum of squares loss: \n",
    "\n",
    "$$\n",
    "L(\\theta) = \\sum \\delta^2 \\ ,\n",
    "$$\n",
    "\n",
    "with a batch of transitions $(s,a,r,s')$ sampled from an experience replay memory. \n",
    "\n",
    "Notice how the DQN implementation uses a different set of parameters, $\\theta'$, to compute the target value. This is the target network parameters which are a delayed copy of $\\theta$.\n",
    "The implementation below uses \"soft\" target updates rather than copying the weights periodically:\n",
    "\n",
    "$$\n",
    "\\theta' \\leftarrow \\tau \\, \\theta + (1-\\tau) \\, \\theta' \\ ,\n",
    "$$\n",
    "\n",
    "with $\\tau \\ll 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show initial state\n",
    "s = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an experience replay memory that can be used to store new transitions and sample mini-batches of previous transitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-network is very similar to the one we have seen previously, but we add the possibility to update the parameters, so the same class can also be used as a target network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.out = nn.Linear(n_inputs, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "    \n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "    \n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoder for the states\n",
    "def one_hot(i, l):\n",
    "    a = np.zeros((len(i), l))\n",
    "    a[range(len(i)), i] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we create a policy network and copy its weight parameters to a target network, so they are initially the same. \n",
    "We also set up a replay memory and prefill it with random transitions sampled from the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Deep Q-network\n",
    "\n",
    "num_episodes = 1000\n",
    "episode_limit = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "gamma = 0.99 # discount rate\n",
    "tau = 0.01 # target network update rate\n",
    "replay_memory_capacity = 10000\n",
    "prefill_memory = True\n",
    "val_freq = 100 # validation frequency\n",
    "\n",
    "n_inputs = env.observation_space.n\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "# initialize DQN and replay memory\n",
    "policy_dqn = DQN(n_inputs, n_outputs, learning_rate)\n",
    "target_dqn = DQN(n_inputs, n_outputs, learning_rate)\n",
    "target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "# prefill replay memory with random actions\n",
    "if prefill_memory:\n",
    "    print('prefill replay memory')\n",
    "    s = env.reset()\n",
    "    while replay_memory.count() < replay_memory_capacity:\n",
    "        a = env.action_space.sample()\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        replay_memory.add(s, a, r, s1, d)\n",
    "        s = s1 if not d else env.reset()\n",
    "        \n",
    "# training loop\n",
    "try:\n",
    "    print('start training')\n",
    "    epsilon = 1.0\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    for i in range(num_episodes):\n",
    "        # init new episode\n",
    "        s, ep_reward, ep_loss = env.reset(), 0, 0\n",
    "        for j in range(episode_limit):\n",
    "            # select action with epsilon-greedy strategy\n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    a = policy_dqn(torch.from_numpy(one_hot([s], n_inputs)).float()).argmax().item()\n",
    "            # perform action\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            # store experience in replay memory\n",
    "            replay_memory.add(s, a, r, s1, d)\n",
    "            # batch update\n",
    "            if replay_memory.count() >= batch_size:\n",
    "                # sample batch from replay memory\n",
    "                batch = np.array(replay_memory.sample(batch_size), dtype=int)\n",
    "                ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
    "                # do forward pass of batch\n",
    "                policy_dqn.optimizer.zero_grad()\n",
    "                Q = policy_dqn(torch.from_numpy(one_hot(ss, n_inputs)).float())\n",
    "                # use target network to compute target Q-values\n",
    "                with torch.no_grad():\n",
    "                    # TODO: use target net\n",
    "                    Q1 = target_dqn(torch.from_numpy(one_hot(ss1, n_inputs)).float())\n",
    "                # compute target for each sampled experience\n",
    "                q_targets = Q.clone()\n",
    "                for k in range(batch_size):\n",
    "                    q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "                # update network weights\n",
    "                loss = policy_dqn.loss(Q, q_targets)\n",
    "                loss.backward()\n",
    "                policy_dqn.optimizer.step()\n",
    "                # update target network parameters from policy network parameters\n",
    "                target_dqn.update_params(policy_dqn.state_dict(), tau)\n",
    "            else:\n",
    "                loss = 0\n",
    "            # bookkeeping\n",
    "            s = s1\n",
    "            ep_reward += r\n",
    "            ep_loss += loss.item()\n",
    "            if d: break\n",
    "        # bookkeeping\n",
    "        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if (i+1) % val_freq == 0: print('%5d mean training reward: %5.2f' % (i+1, np.mean(rewards[-val_freq:])))\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(411)\n",
    "plt.title('training rewards')\n",
    "plt.plot(range(1, num_episodes+1), rewards)\n",
    "plt.plot(moving_average(rewards))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(412)\n",
    "plt.title('training lengths')\n",
    "plt.plot(range(1, num_episodes+1), lengths)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(lengths))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(413)\n",
    "plt.title('training loss')\n",
    "plt.plot(range(1, num_episodes+1), losses)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(losses))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(414)\n",
    "plt.title('epsilon')\n",
    "plt.plot(range(1, num_episodes+1), epsilons)\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though here we have applied DQN to a rather simple problem, hopefully you can see how this can be scaled up to solve more challenging tasks.\n",
    "For example by adding convolutional layers to the network and learning to play Atari games!\n",
    "\n",
    "## Popular extensions of DQN\n",
    "\n",
    "After the initial success of DQN, the method as been extended in a number of ways. Some popular improvements are:\n",
    "\n",
    "* [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)\n",
    "* [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)\n",
    "* [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Make sure you read and understand the code, and feel free to experiment with the:\n",
    "\n",
    "* number of episodes\n",
    "* discount factor\n",
    "* learning rate\n",
    "* network layers\n",
    "\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "*Did you see any improvements over the regular Q-network from the previous notebook? Why/why not?*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 2 \n",
    "\n",
    "*Solve another (harder) environment from OpenAI Gym with DQN. Describe the choices you made and what you learned.*\n",
    "\n",
    "*Hint: If you for example want to master a Atari game, add convolutional layers to the network. *\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "*Improve the DQN algorithm with one or more of the popular extensions described above. Describe the choices you made and what you learned.*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
