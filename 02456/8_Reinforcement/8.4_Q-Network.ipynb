{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve frozen lake with Q-network\n",
    "\n",
    "> By Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk))\n",
    "\n",
    "Now we will solve the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment using on-policy neural Q-learning with epsilon-greedy action selection.\n",
    "\n",
    "In the FrozenLake problem the reinforcement learning agent must navigate a small grid world by moving up, down, left and right. \n",
    "The task is to move from start position (S) to the goal (G) whithout falling into a hole (H). \n",
    "The task is made more difficult by the wind that will sometimes send the agent in a random direction regardless of the chosen action.\n",
    "See an example start state below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-network\n",
    "\n",
    "The method we will be using employs a Q-function which can be updated using bootstrapping:\n",
    "\n",
    "$$\n",
    "Q(s,a;\\theta) \\leftarrow Q(s,a;\\theta) + \\alpha \\delta \\ ,\n",
    "$$\n",
    "\n",
    "where $\\delta = r + \\gamma \\max_{a'} Q(s',a';\\theta) - Q(s,a;\\theta)$ is the TD error.\n",
    "\n",
    "Since we use a neural network to represent $Q$, we cannot do this assignment directly, but instead perform a gradient update using squared loss: \n",
    "\n",
    "$$\n",
    "L(\\theta) = \\delta^2 \\ ,\n",
    "$$\n",
    "\n",
    "with transitions $(s,a,r,s')$ sampled from the environment. \n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Do a forward pass for current state, $s$, to get predicted Q-values for all actions.\n",
    "2. Select an action, $a$, with epsilon-greedy strategy and execute it to observe a reward and next state, $r,s'$.\n",
    "3. Do a forward pass for the next state, $s'$.\n",
    "4. Set Q-target for action $a$ to $r + \\gamma \\max_{a'} Q(s',a';\\theta)$ and to the Q-values from step 1 for all other actions, making the TD error zero for those outputs.\n",
    "5. Update network weights with backpropagation.\n",
    "6. Set $s\\leftarrow s'$ and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize the FrozenLake environment and display an example of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show init state\n",
    "s = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's implement the Q-network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-network\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # network\n",
    "        self.out = nn.Linear(n_inputs, n_outputs, bias=False)\n",
    "        torch.nn.init.uniform_(self.out.weight, 0, 0.01)\n",
    "        # training\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "    \n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(i, l):\n",
    "    \"\"\"One-hot encoder for the states\"\"\"\n",
    "    a = np.zeros((len(i), l))\n",
    "    a[range(len(i)), i] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Q-network\n",
    "\n",
    "num_episodes = 1000\n",
    "episode_limit = 100\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99 # discount rate\n",
    "val_freq = 100 # validation frequency\n",
    "epsilon_start = 1.0\n",
    "\n",
    "n_inputs = env.observation_space.n\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "qnet = QNetwork(n_inputs, n_outputs, learning_rate)\n",
    "\n",
    "try:\n",
    "    epsilon = epsilon_start\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "    print('start training')\n",
    "    for i in range(num_episodes):\n",
    "        # init new episode\n",
    "        s, ep_reward, ep_loss = env.reset(), 0, 0\n",
    "        for j in range(episode_limit):\n",
    "            # 1. do foward pass of current state to compute Q-values for all actions\n",
    "            qnet.optimizer.zero_grad()\n",
    "            Q = qnet(torch.from_numpy(one_hot([s], n_inputs)).float())\n",
    "            # 2. select action with epsilon-greedy strategy\n",
    "            a = Q.argmax().item() if np.random.rand() > epsilon else env.action_space.sample()\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            # 3. do forward pass for the next state\n",
    "            with torch.no_grad():\n",
    "                Q1 = qnet(torch.from_numpy(one_hot([s1], n_inputs)).float())\n",
    "            # 4. set Q-target\n",
    "            q_target = Q.clone()\n",
    "            q_target[0, a] = r + gamma * Q1.max().item() * (not done)\n",
    "            # 5. update network weights\n",
    "            loss = qnet.loss(Q, q_target)\n",
    "            loss.backward()\n",
    "            qnet.optimizer.step()\n",
    "            # 6. bookkeeping\n",
    "            s = s1\n",
    "            ep_reward += r\n",
    "            ep_loss += loss.item()\n",
    "            if done: break\n",
    "        # bookkeeping\n",
    "        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "        if (i+1) % val_freq == 0: print('{:5d} mean training reward: {:5.2f}'.format(i+1, np.mean(rewards[-val_freq:])))\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(411)\n",
    "plt.title('training rewards')\n",
    "plt.plot(range(1, num_episodes+1), rewards)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(rewards))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(412)\n",
    "plt.title('training lengths')\n",
    "plt.plot(range(1, num_episodes+1), lengths)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(lengths))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(413)\n",
    "plt.title('training loss')\n",
    "plt.plot(range(1, num_episodes+1), losses)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(losses))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(414)\n",
    "plt.title('epsilon')\n",
    "plt.plot(range(1, num_episodes+1), epsilons)\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review the solution! You can run the cell multiple times to see the behavior of the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "env.render()\n",
    "for _ in range(100):\n",
    "    a = qnet(torch.from_numpy(one_hot([s], n_inputs)).float()).argmax().item()\n",
    "    s, r, done, _ = env.step(a)\n",
    "    env.render()\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Read and understand the code, then play around with it and try to make it learn better and faster.\n",
    "\n",
    "Experiment with the:\n",
    "\n",
    "* number of episodes\n",
    "* discount factor\n",
    "* learning rate\n",
    "* network layers\n",
    "\n",
    "\n",
    "### Exercise 1 \n",
    "\n",
    "*Describe any changes you made to the code and why you think they improve the agent.*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "*How high mean training a reward is your solution able to achieve? Do you think it is possible to go even higher? Why/why not?*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "*What role does epsilon play in the code above? Try and change the epsilon start value or the line of code that decreases eplison every update step. How does it affect learning?*\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "*Answer here...*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
