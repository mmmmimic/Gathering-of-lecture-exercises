{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "This is an optional supplementary notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "plt.style.use([\"seaborn-deep\", \"seaborn-whitegrid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The density problem\n",
    "\n",
    "Similarly to the VAE and GAN, we are interested in capturing some empirical input distribution $p(x)$, for which we observe a dataset of samples $\\{ x\\}^n_{i=1} \\sim p(x)$. Each of the models we have seen have been capable of performing this task, but there are a number of problems with both approaches:\n",
    "\n",
    "A major weakness of the VAE was that it does not generate \"sharp\" images even if we average over multiple samples. The reason for this is grounded in the maximisation of the lower bound, which is only a surrogate for the generative quality, as it is heavily penalised by the KL-term.\n",
    "\n",
    "For GANs we saw sharp images, but many other problems were swept under the rug. Firstly, training GANs is extremely difficult, with the models being prone to mode-collapses and divergence during training. Additionally, the probabilistic framework posited by VAEs, gives us way to measure the probability of a sample under the distribution - this is missing in GANs.\n",
    "\n",
    "Luckily, another recent technique has shown promise on these fronts. Let us preface the model by considering how one would estimate the density of a random variable under a known transform. If we let $z \\sim \\mathcal{N}(0, 1)$ and $f(z) = (z/2)^3$ we can plot the resulting density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "device = \"cuda:0\" if cuda else \"cpu\"\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "n_samples = 5000\n",
    "\n",
    "# Define the base distribution p(z)\n",
    "base = Normal(torch.zeros(2), torch.ones(2))\n",
    "z = base.sample((n_samples,))\n",
    "\n",
    "ax1.scatter(*z.t(), s=3)\n",
    "ax1.set_title(r\"Samples $z \\sim N(0, I)$\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Na√Øvely transform samples\n",
    "f = lambda z: (z/3)**3\n",
    "x = f(z)\n",
    "\n",
    "ax2.scatter(*x.t(), s=3)\n",
    "ax2.set_title(r\"$x = f(z)$\")\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neat trick from probability theory to estimate the density under $f$ is the change of variables formula. For any injective function $f$, we know the following.\n",
    "\n",
    "$$p(x) = p(z) \\left | \\det \\left( \\frac{\\partial f(z)^{-1}}{\\partial z} \\right )\\right | = p(z) \\left | \\det \\left(\\frac{\\partial f(z)}{\\partial z} \\right ) \\right |^{-1}$$\n",
    "\n",
    "Where $\\partial f(z)/\\partial z$ represents the Jacobian of the transform. With this in mind we can fully characterise $p(x)$ including sampling and scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Assignment**:\n",
    "\n",
    "Use the change of variables formula above to compute $p(x)$ analytically knowing that $z$ is a standard bivariate normal distribution and $f(z) = (z/2)^3$. Is $f$ invertible?\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this formula we can estimate any density as long as we have a known base distribution $p(z)$ and are able to compute the Jacobian matrix of the transform we are interested in. So let us therefore consider a more difficult example seen below. It is known that there exists some function from $z$ to this space, but we do not know *what* it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = plt.figure(figsize=(6, 6))\n",
    "n_samples = 500\n",
    "\n",
    "l = torch.linspace(-np.pi, np.pi, n_samples)\n",
    "x = torch.stack([torch.cos(l), torch.sin(l)]).t() + 0.1 * torch.randn(n_samples, 2)\n",
    "\n",
    "plt.scatter(*x.t(), s=3)\n",
    "plt.title(r\"Donut distribution $p_x$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow models\n",
    "\n",
    "Most likely, you are at wit's end when it comes to figuring out what $f$ could be in this case, so let us instead use deep learning. The idea is simple, let a neural network act as $f: Z \\to X$ and constrain the network such that the Jacobian is tractable. The model class has many different names, but is often referred to as **Flow** models after one of the most popular papers on the subject, *normalizing flows* [[Rezende & Mohamed, 2015]](https://arxiv.org/abs/1505.05770), though the idea is much older.\n",
    "\n",
    "In the paper they solve the problem by making a neural network consisting of layers of invertible transformations for which the determinant of the Jacobian can be computed in linear time. The exact formulation of these flows is very cumbersome and the implementation slow, so we will instead work on the model described by [[Dinh, 2016]](https://arxiv.org/abs/1605.08803)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupling Layers\n",
    "\n",
    "Coupling layers proposed in [[Dinh, 2014]](https://arxiv.org/abs/1410.8516) provides a principled way of computing the determinant of the Jacobian in linear time by making the layer itself invertible. The layer splits the input into two streams and perform a translation and scaling of one stream, the other stays constant. Because one stream is unchanged, the Jacobian will be triangular.\n",
    "\n",
    "*Forward propagation* through the layer is shown below.\n",
    "\n",
    "<img src=\"https://hci.iwr.uni-heidelberg.de/vislearn/wp-content/uploads/2018/07/INN-coupling-layer.png\" alt=\"Coupling layer\" width=\"500px\">\n",
    "\n",
    "*Inverse propagation* through the layer is simple equally simple.\n",
    "\n",
    "<img src=\"https://hci.iwr.uni-heidelberg.de/vislearn/wp-content/uploads/2018/07/INN-coupling-layer-inverse.png\" alt=\"Coupling layer inverse\" width=\"500px\">\n",
    "\n",
    "Here are some quick facts about the layer to motivate its use.\n",
    "\n",
    "* The Jacobian is lower-triangular, therefore the determinant is given as the product of the diagonal elements $\\prod_{i=1}^d s_i$.\n",
    "* Layer stacking is trivial due to the identity $\\det(AB) = \\det(A)\\det(B)$.\n",
    "* The inner neural modules (s, t) can be arbitrarily complex\n",
    "* $f$ must be bijective, so it is required that $dim(X) = dim(Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    \"\"\"\n",
    "    Affine coupling layer described in \"RealNVP\" [Dinh et al, 2016]\n",
    "    \n",
    "    A mask is used to split the streams\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=256):\n",
    "        super(AffineCoupling, self).__init__()\n",
    "\n",
    "        self.scale = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_features, in_features),\n",
    "        )\n",
    "        \n",
    "        self.translate = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_features, in_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward propagation (inference) y = f(x) = x1 + (x2 * s(x1) + t(x1))\n",
    "        \"\"\" \n",
    "        x_ = mask * x\n",
    "\n",
    "        s = self.scale(x_) * (1 - mask)\n",
    "        t = self.translate(x_) * (1 - mask)\n",
    "\n",
    "        y = x_ + (1 - mask) * (x * torch.exp(s) + t)\n",
    "        jacobian = torch.sum(s, -1, keepdim=True)\n",
    "        \n",
    "        return y, jacobian\n",
    "\n",
    "    def inverse(self, y, mask=None):\n",
    "        \"\"\"\n",
    "        Inverse propagation (generation) x = f^(-1)(y) = y1 + (y2 - t(y1)) / s(y1))\n",
    "        \"\"\"\n",
    "        y_ = mask * y\n",
    "\n",
    "        s = self.scale(y_) * (1 - mask)\n",
    "        t = self.translate(y_) * (1 - mask)\n",
    "\n",
    "        x = y_ + (1 - mask) * ((y - t) * torch.exp(-s))\n",
    "        jacobian = torch.sum(s, -1, keepdim=True)\n",
    "\n",
    "        return x, jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Real non-volume preserving network (RealNVP)\n",
    "    is just a stack of affine coupling layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=256, depth=6):\n",
    "        super(RealNVP, self).__init__()\n",
    "        self.layers = nn.ModuleList([AffineCoupling(in_features, hidden_features) for _ in range(depth)])\n",
    "        \n",
    "        # A binary mask is used to make splitting of streams simple\n",
    "        base_mask = torch.FloatTensor([[0, 1]])\n",
    "        self.mask = nn.Parameter(base_mask.repeat((depth, 1)), requires_grad=False)\n",
    "        \n",
    "        # Alternate masking\n",
    "        indices = [i for i in range(depth) if i%2]\n",
    "        self.mask[indices] = 1 - self.mask[indices]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation (inference) y = f(x)\n",
    "        \n",
    "        Gather the jacobian, log det(ab) = log det(a) + log det(b)\n",
    "        \"\"\"\n",
    "        jacobian = torch.zeros_like(x)\n",
    "        for mask, layer in zip(self.mask, self.layers):\n",
    "            x, log_det_J = layer.forward(x, mask)\n",
    "            jacobian += log_det_J\n",
    "        \n",
    "        return x, jacobian\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        \"\"\"\n",
    "        Inverse propagation (generation) x = f^(-1)(y)\n",
    "        \"\"\"\n",
    "        jacobian = torch.zeros_like(y)\n",
    "        # Run the operation in reverse\n",
    "        for mask, layer in zip(reversed(self.mask), reversed(self.layers)):\n",
    "            y, log_det_J = layer.inverse(y, mask)\n",
    "            jacobian -= log_det_J\n",
    "\n",
    "        return y, jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base distribution and its parameters\n",
    "mu = torch.zeros(2).to(device)\n",
    "sigma = torch.ones(2).to(device)\n",
    "base = Normal(mu, sigma)\n",
    "\n",
    "# The RealNVP model is a stack of identical coupling layers, here we use 6 couplng layers.\n",
    "model = RealNVP(in_features=2, hidden_features=256, depth=6).to(device)\n",
    "optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a loss function we can simply resort to maximum likelihood, or equivalently, minimising the negative log likelihood.\n",
    "\n",
    "$$-\\max_\\theta \\log p(x) = -\\log p(z) - \\log \\left | \\det \\left( \\frac{\\partial f_{\\theta}(z)^{-1}}{\\partial z} \\right ) \\right |$$\n",
    "\n",
    "Below you will see how the network learns both forward and inverse propagation, making it able to generate data from both the latent distribution and the input distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tmp_img = \"tmp_nvp_out.png\"\n",
    "losses = []\n",
    "\n",
    "x = x.to(device)\n",
    "\n",
    "for epoch in range(2500):\n",
    "    z, log_det_J = model(x)\n",
    "    \n",
    "    # Loss function\n",
    "    loss = -torch.mean(base.log_prob(z) + log_det_J)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # -- Plotting --\n",
    "    if epoch % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            z = base.sample((n_samples,))\n",
    "            z_hat, _ = model.forward(x)\n",
    "            x_hat, _ = model.inverse(z)\n",
    "        \n",
    "        f, axarr = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "        # Loss\n",
    "        ax = axarr[0]\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Negative log likelihood')\n",
    "        ax.plot(np.arange(epoch+1), losses, color=\"black\")\n",
    "\n",
    "        # Data space\n",
    "        ax = axarr[1]\n",
    "        ax.set_title(\"Data space\")\n",
    "        ax.scatter(*x.data.cpu().t(), s=3, c=\"k\", label=\"True\")\n",
    "        ax.scatter(*x_hat.data.cpu().t(), s=3, label=\"Learned\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim([-3, 3]); ax.set_ylim([-3, 3])\n",
    "\n",
    "        # Latent space\n",
    "        ax = axarr[2]\n",
    "        ax.set_title(\"Latent space\")\n",
    "        ax.scatter(*z.data.cpu().t(), s=3, c=\"k\", label=\"True\")\n",
    "        ax.scatter(*z_hat.data.cpu().t(), s=3, label=\"Learned\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim([-3, 3]); ax.set_ylim([-3, 3])\n",
    "\n",
    "        plt.savefig(tmp_img)\n",
    "        plt.close(f)\n",
    "        display(Image(filename=tmp_img))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        os.remove(tmp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment**:\n",
    "    \n",
    "Experiment with the model. How many affine layers and total neurons do we need to solve this problem? *Hint: You can use the code block below to calculate this number.*\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total neurons:\", sum([np.prod(p.shape) for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is possible with this class of models? To get the answer have a look at [OpenAI's](https://blog.openai.com/glow/) recent blog post on the subject.**\n",
    "\n",
    "In this week we have now discussed all of the current major directions within generative modelling with the exception of autoregressive models. If you are interested in understanding these, please look into models such as PixelCNN and Wavenet. The following table is originally by Laurent Dinh and is a good place to start when comparing the different approaches to deep generative modelling.\n",
    "\n",
    "|-|Autoregressive|Variational Autoencoders|Generative Adversarial Networks|Flows|\n",
    "|-|--------------|---|---|---|\n",
    "|Objective|log-likelihood (stable)|doubly stochastic ELBO (stable)|approximate adversarial loss (unstable)|log-likelihood (stable)|\n",
    "|Latent space|None|dimension collapsing|low dimensional|high-dimensional|\n",
    "|Architecture|requires ordering, arbitrary data|arbitrary|arbitrary, continuous data|requires partitioning (NVP), continuous data|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
